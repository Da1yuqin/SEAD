#!/bin/bash
# scripts/train_chatbot.sh
# ================================================
# Chatbot Training Script
# ================================================


set -e

chatbot_model_path=$1
client_model_path=$2
model_name=$3
step=$4
training_steps=${5:-10}
keep_vllm_alive=${6:-false}

if [ -z "$chatbot_model_path" ] || [ -z "$client_model_path" ] || [ -z "$model_name" ] || [ -z "$step" ]; then
    echo "‚ùå Error: Missing required parameters"
    echo "Usage: bash $0 <chatbot_model_path> <client_model_path> <model_name> <step> [training_steps] [keep_vllm_alive]"
    exit 1
fi

RUN_ID=$(date +%s%N)
export RUN_ID
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# ‚úÖ Read configuration from environment variables (if set by main.sh), otherwise use default values
TRAIN_BATCH_SIZE=${TRAIN_BATCH_SIZE:-60}
VAL_BATCH_SIZE=${VAL_BATCH_SIZE:-60}
TEST_FREQ=${TEST_FREQ:-1}

echo "=========================================="
echo "üìã Training Parameters"
echo "=========================================="
echo "  Chatbot Model: $chatbot_model_path"
echo "  Client Model: $client_model_path"
echo "  Output Name: $model_name"
echo "  Iteration Step: $step"
echo "  Training Steps: $training_steps"
echo "  Train Batch Size: $TRAIN_BATCH_SIZE"
echo "  Validation Batch Size: $VAL_BATCH_SIZE"
echo "  Test Frequency: Every $TEST_FREQ steps"
echo "  Keep vLLM Running: $keep_vllm_alive"
echo "=========================================="
echo ""

# ‚úÖ Check if vLLM Server is already running
echo "üîç Checking vLLM Server status..."
VLLM_ALREADY_RUNNING=false

if curl -s -f http://localhost:5000/health > /dev/null 2>&1; then
    echo "‚úÖ vLLM Server is already running, skipping startup"
    VLLM_ALREADY_RUNNING=true
else
    echo "üöÄ Need to start vLLM Server..."
    
    # Clean up existing services
    echo "üßπ Cleaning up existing vLLM services..."
    pkill -9 -f "vllm_server" 2>/dev/null || true
    sleep 2
    
    source your/path/to/conda.sh
    conda activate vllm
    

    echo ""
    echo "üöÄ Starting vLLM Server (User Role-play Model, GPU 6-7)..."
    bash ./SEAD/vllm_service/start_vllm.sh "$client_model_path" "$RUN_ID" "$step" "call_client" 5000
    
    echo ""
    echo "‚è≥ Waiting for vLLM Server to start..."
    MAX_WAIT=99999
    WAIT_COUNT=0
    
    while [ $WAIT_COUNT -lt $MAX_WAIT ]; do
        if curl -s -f http://localhost:5000/health > /dev/null 2>&1; then
            echo "‚úÖ vLLM Server is ready!"
            break
        fi
        
        echo "   Waiting... (${WAIT_COUNT}s / ${MAX_WAIT}s)"
        sleep 5
        WAIT_COUNT=$((WAIT_COUNT + 5))
    done
    
    if [ $WAIT_COUNT -ge $MAX_WAIT ]; then
        echo ""
        echo "‚ùå Error: vLLM Server startup timeout"
        tail -100 ./outputs/logs/vllm_server/call_client_server_step${step}.log
        exit 1
    fi
fi

echo ""
echo "üîÑ Switching to training environment..."
conda activate SEAD


export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5
export VLLM_ATTENTION_BACKEND=XFORMERS

# ‚úÖ Validate training data (generated by main.sh)
echo ""
echo "üîç Validating training data..."

WORK_DIR=$(pwd)
TRAIN_DATA="${WORK_DIR}/outputs/prompt_data/train_chatbot.parquet"
VAL_DATA="${WORK_DIR}/outputs/prompt_data/test_chatbot.parquet"

if [ ! -f "$TRAIN_DATA" ]; then
    echo "‚ùå Error: Training data does not exist: $TRAIN_DATA"
    echo "   Data should be generated by main.sh"
    exit 1
fi

if [ ! -f "$VAL_DATA" ]; then
    echo "‚ùå Error: Validation data does not exist: $VAL_DATA"
    echo "   Data should be generated by main.sh"
    exit 1
fi

echo "‚úÖ Data files exist"

# ‚úÖ Validate data volume
python3 << EOF
import pandas as pd
import sys

try:
    train_df = pd.read_parquet('$TRAIN_DATA')
    val_df = pd.read_parquet('$VAL_DATA')
    
    print(f"‚úÖ Training set: {len(train_df)} rows")
    print(f"‚úÖ Validation set: {len(val_df)} rows")
    
    if len(train_df) == 0:
        print("‚ùå Training set is empty")
        sys.exit(1)
    
    if len(val_df) == 0:
        print("‚ùå Validation set is empty")
        sys.exit(1)
    
    # Check if data volume is sufficient
    if len(train_df) < $TRAIN_BATCH_SIZE:
        print(f"‚ùå Error: Training set too small ({len(train_df)} < $TRAIN_BATCH_SIZE)")
        sys.exit(1)
    
    if len(val_df) < $VAL_BATCH_SIZE:
        print(f"‚ùå Error: Validation set too small ({len(val_df)} < $VAL_BATCH_SIZE)")
        sys.exit(1)
    
    print("‚úÖ Data volume is sufficient")
    
except Exception as e:
    print(f"‚ùå Data validation failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
EOF

if [ $? -ne 0 ]; then
    echo "‚ùå Data validation failed"
    exit 1
fi

# ‚úÖ Validate user_params files
echo ""
echo "üìä Validating user_params files:"
if [ -f "./outputs/temp/train_client_prompts.jsonl" ]; then
    ACTUAL_TRAIN=$(wc -l < ./outputs/temp/train_client_prompts.jsonl)
    echo "   train_client_prompts.jsonl: ${ACTUAL_TRAIN} entries"
else
    echo "   ‚ö†Ô∏è  train_client_prompts.jsonl not found"
fi

if [ -f "./outputs/temp/test_client_prompts.jsonl" ]; then
    ACTUAL_TEST=$(wc -l < ./outputs/temp/test_client_prompts.jsonl)
    echo "   test_client_prompts.jsonl: ${ACTUAL_TEST} entries"
else
    echo "   ‚ö†Ô∏è  test_client_prompts.jsonl not found"
fi

# ‚úÖ Reset indices
echo ""
echo "üîÑ Resetting indices..."
echo "0" > ./outputs/temp/train_index.txt
echo "0" > ./outputs/temp/test_index.txt
echo "‚úÖ Indices have been reset"

# ‚úÖ Notify vLLM Server to reset indices
echo ""
echo "üîÑ Notifying vLLM Server to reset indices..."
curl -X POST http://localhost:5000/reset_index \
    -H "Content-Type: application/json" \
    -d '{"train": true, "test": true}' 2>/dev/null || echo "‚ö†Ô∏è  Cannot connect to vLLM Server"

echo ""
echo "üéì Starting Chatbot training..."
LOG_FILE="./outputs/logs/chatbot_train_step${step}.log"
mkdir -p ./outputs/logs


PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
    data.train_files=$TRAIN_DATA \
    data.val_files=$VAL_DATA \
    data.train_batch_size=$TRAIN_BATCH_SIZE \
    data.val_batch_size=$VAL_BATCH_SIZE \
    data.max_prompt_length=8192 \
    data.max_response_length=500 \
    data.max_start_length=2048 \
    data.max_obs_length=500 \
    data.shuffle_train_dataloader=True \
    algorithm.adv_estimator=grpo \
    actor_rollout_ref.model.path=${chatbot_model_path} \
    actor_rollout_ref.model.enable_gradient_checkpointing=true \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.1 \
    actor_rollout_ref.actor.use_kl_loss=true \
    actor_rollout_ref.actor.ppo_mini_batch_size=60 \
    actor_rollout_ref.actor.ppo_micro_batch_size=6 \
    actor_rollout_ref.actor.fsdp_config.param_offload=false \
    actor_rollout_ref.actor.fsdp_config.grad_offload=false \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=true \
    actor_rollout_ref.rollout.log_prob_micro_batch_size=6 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.ref.log_prob_micro_batch_size=6 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    algorithm.no_think_rl=false \
    actor_rollout_ref.rollout.n_agent=8 \
    actor_rollout_ref.rollout.temperature=1.2 \
    actor_rollout_ref.actor.state_masking=true \
    trainer.logger=['console'] \
    +trainer.val_only=false \
    +trainer.val_before_train=false \
    trainer.default_hdfs_dir=null \
    trainer.n_gpus_per_node=6 \
    trainer.nnodes=1 \
    trainer.save_freq=5 \
    trainer.test_freq=$TEST_FREQ \
    trainer.project_name=$WAND_PROJECT \
    trainer.experiment_name=$EXPERIMENT_NAME \
    trainer.total_epochs=1 \
    trainer.total_training_steps=$training_steps \
    trainer.default_local_dir=./${STORAGE_PATH}/models/$model_name \
    chat_server.url="http://localhost:5000/" \
    chat_server.request_timeout_s=9999999 \
    chat_server.batch_size=30 \
    chat_server.use_mock=False \
    max_turns=15 \
    multi_turn_chat=True \
    actor_rollout_ref.rollout.dtype=bfloat16 \
    +generation_mode=call_chatbot \
    2>&1 | tee "$LOG_FILE"


TRAIN_EXIT_CODE=$?

# ‚úÖ First close vLLM (if needed), then check training results
if [ "$keep_vllm_alive" = "false" ]; then
    echo ""
    echo "=========================================="
    echo "üßπ Training completed, shutting down vLLM Server"
    echo "=========================================="
    
    pkill -9 -f "vllm_server" 2>/dev/null || true
    sleep 3
    
    REMAINING_PIDS=$(pgrep -f "vllm_server" || true)
    if [ -n "$REMAINING_PIDS" ]; then
        echo "‚ö†Ô∏è Warning: vLLM Server still running, force terminating..."
        kill -9 $REMAINING_PIDS 2>/dev/null || true
        sleep 2
    fi
    
    rm -rf /tmp/ray_* 2>/dev/null || true
    rm -rf /dev/shm/ray_* 2>/dev/null || true
    
    echo "‚úÖ vLLM Server has been shut down, GPU 6-7 released"
    echo "=========================================="
else
    echo ""
    echo "=========================================="
    echo "‚úÖ Training completed, keeping vLLM Server running"
    echo "=========================================="
    echo "   vLLM Server continues running on GPU 6-7"
    echo "   Port: 5000"
    echo "   Next iteration will reuse this service"
    echo "=========================================="
fi

# ‚úÖ Check training results
if [ $TRAIN_EXIT_CODE -ne 0 ]; then
    echo ""
    echo "‚ùå Training failed! Exit code: $TRAIN_EXIT_CODE"
    echo "View log: $LOG_FILE"
    exit 1
fi

# ‚úÖ Dynamically find latest checkpoint
echo ""
echo "üîç Validating training results..."

MODEL_BASE_DIR="./outputs/models/${model_name}/actor"

if [ ! -d "$MODEL_BASE_DIR" ]; then
    echo "‚ùå Error: Model directory does not exist: $MODEL_BASE_DIR"
    echo ""
    echo "Possible reasons:"
    echo "  1. Error occurred during training"
    echo "  2. Model save path configuration error"
    echo "  3. Insufficient disk space"
    echo ""
    echo "Please check training log: $LOG_FILE"
    tail -50 "$LOG_FILE"
    exit 1
fi

# Find all checkpoints
CHECKPOINTS=$(ls -d ${MODEL_BASE_DIR}/global_step_* 2>/dev/null | sort -V)

if [ -z "$CHECKPOINTS" ]; then
    echo "‚ùå Error: No checkpoints found"
    echo ""
    echo "Directory contents:"
    ls -la "$MODEL_BASE_DIR"
    echo ""
    echo "Please check training log: $LOG_FILE"
    tail -50 "$LOG_FILE"
    exit 1
fi

# Get the latest checkpoint
LATEST_CHECKPOINT=$(echo "$CHECKPOINTS" | tail -1)
CHECKPOINT_STEP=$(basename "$LATEST_CHECKPOINT" | sed 's/global_step_//')

echo "‚úÖ Found checkpoints:"
echo "$CHECKPOINTS" | while read ckpt; do
    echo "   - $(basename $ckpt)"
done
echo ""
echo "‚úÖ Latest checkpoint: global_step_${CHECKPOINT_STEP}"

echo ""
echo "‚úÖ Training completed!"
echo "   Model saved at: $LATEST_CHECKPOINT"
echo "   Log file: $LOG_FILE"
echo ""