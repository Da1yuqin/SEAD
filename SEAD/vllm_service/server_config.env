# server_config.env - vLLM Server Configuration File

# ✅ Model Path (Required)
# Specify the path to your model directory
# Example: "./models/Qwen2.5-14B-Instruct" or "/path/to/your/model"
VLLM_MODEL_PATH="your/path/to/Qwen2.5-14B-Instruct"

# ✅ Run ID (Optional, Default: default)
# Identifier for this training/inference run
# Used for organizing outputs and logs
VLLM_RUN_ID="training_user_sim"

# ✅ Step Number (Optional, Default: 0)
# Training checkpoint step to load
# Set to 0 for base model, or specific step number for checkpoint
VLLM_STEP="0"

# ✅ Mode (Optional, Default: call_chatbot)
# Operation mode for the server
# Options: "call_client" or "call_chatbot"
VLLM_MODE="call_client"

# ✅ Port (Optional, Default: 5000)
# Server listening port
# Make sure this port is available and not blocked by firewall
VLLM_PORT="5000"

