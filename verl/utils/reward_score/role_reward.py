import re
from typing import Dict, List, Tuple, Optional
import json
import os
from collections import Counter
import numpy as np
import random
import logging
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time

logger = logging.getLogger(__name__)

# ==================== Helper Functions ====================

def _extract_tag_value(text: str, tag_name: str, cast_type=None):
    """Extract value of a specific tag from text"""
    pattern = f'<{tag_name}[^>]*>(.*?)</{tag_name}>'
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match:
        value_str = match.group(1).strip()
        if cast_type:
            try:
                return cast_type(value_str)
            except (ValueError, TypeError):
                return None
        return value_str
    return None

def _extract_user_final_status_from_turns(turns: List[Dict]) -> int:
    """
    Extract user final status from parsed turns.
    Note: turns is structured data generated by _parse_ground_truth_history,
    containing 'user_status' field, not raw 'role'/'content'.
    """
    if not turns:
        return None
        
    # Traverse backwards to find the last turn containing user_status
    for turn in reversed(turns):
        # Directly get parsed user_status (int)
        status = turn.get('user_status')
        if status is not None:
            return status
            
    return None

def _parse_ground_truth_history(ground_truth_history: List[Dict]) -> Dict:
    """
    Extract trajectory data from ground_truth_history
    """
    result = {
        "turns": [],
        "raw_statuss": [],
        "model_statuss": [],
        "parse_status": "success"
    }
    
    current_turn = {}
    turn_id = 0
    
    for i, item in enumerate(ground_truth_history):
        role = item.get('role')
        content = item.get('content', '')
        
        if role == 'assistant':
            # If the current turn already has a user_reply, the previous round is complete; save it
            if current_turn and current_turn.get('user_reply'):
                result["turns"].append(current_turn)
                # Pair only when both model_status and user_status exist
                if current_turn.get('model_status') is not None and current_turn.get('user_status') is not None:
                    result["model_statuss"].append(current_turn['model_status'])
                    result["raw_statuss"].append(current_turn['user_status'])
                current_turn = {}
            
            # Start a new turn
            if not current_turn:
                turn_id += 1
                current_turn = {'turn_id': turn_id}
            
            # Extract response
            response_match = re.search(r'<response[^>]*>(.*?)</response>', content, re.DOTALL | re.IGNORECASE)
            if response_match:
                current_turn['response'] = response_match.group(1).strip()
            
            # Extract Chatbot status
            for tag, key in [
                ('cooperation_score', 'cooperation_score'),
                ('emotion_score', 'emotion_score'),
                ('trust_score', 'trust_score'),
                ('noise_score', 'noise_score'),
                ('stage', 'stage'),
                ('status', 'model_status'),
            ]:
                value = _extract_tag_value(content, tag, int)
                if value is not None:
                    current_turn[key] = value
            
            # If this is the last message (Chatbot hung up proactively)
            is_last_message = (i == len(ground_truth_history) - 1)
            if is_last_message and current_turn.get('model_status') is not None:
                # Save this turn (even without user_reply)
                result["turns"].append(current_turn)
                
                # Use the user_status from the previous round
                if result["raw_statuss"]:
                    prev_user_status = result["raw_statuss"][-1]
                    result["model_statuss"].append(current_turn['model_status'])
                    result["raw_statuss"].append(prev_user_status)
        
        elif role == 'user':
            # If current_turn does not exist yet, create one
            if not current_turn:
                turn_id += 1
                current_turn = {'turn_id': turn_id}
            
            # Extract user_reply
            user_reply_match = re.search(r'<user_reply[^>]*>(.*?)</user_reply>', content, re.DOTALL | re.IGNORECASE)
            if user_reply_match:
                current_turn['user_reply'] = user_reply_match.group(1).strip()
            
            # Extract user_think
            user_think_match = re.search(r'<user_think[^>]*>(.*?)</user_think>', content, re.DOTALL | re.IGNORECASE)
            if user_think_match:
                current_turn['user_think'] = user_think_match.group(1).strip()
            
            # Extract User status
            for tag, key in [
                ('user_cooperation', 'user_cooperation'),
                ('user_emotion', 'user_emotion'),
                ('user_trust', 'user_trust'),
                ('user_noise', 'user_noise'),
                ('user_status', 'user_status'),
            ]:
                value = _extract_tag_value(content, tag, int)
                if value is not None:
                    current_turn[key] = value
    
    # Process the last turn (if not saved yet)
    if current_turn and current_turn.get('user_reply'):
        result["turns"].append(current_turn)
        if current_turn.get('model_status') is not None and current_turn.get('user_status') is not None:
            result["model_statuss"].append(current_turn['model_status'])
            result["raw_statuss"].append(current_turn['user_status'])
    
    return result


def _process_turns_for_display(turns: List[Dict], mode: str = 'call_chatbot') -> List[Dict]:
    """Process trajectory data for display"""
    processed_turns = []
    for turn in turns:
        processed_turn = {
            "turn_id": turn.get("turn_id"),
            "response": turn.get("response"),
            "user_reply": turn.get("user_reply"),
            "user_status": turn.get("user_status"),
        }
        
        if mode == 'call_client':
            if any([
                turn.get("user_think"),
                turn.get("user_cooperation") is not None,
                turn.get("user_emotion") is not None,
                turn.get("user_trust") is not None,
                turn.get("user_noise") is not None,
            ]):
                processed_turn["user_states"] = {
                    "think": turn.get("user_think"),
                    "cooperation": turn.get("user_cooperation"),
                    "emotion": turn.get("user_emotion"),
                    "trust": turn.get("user_trust"),
                    "noise": turn.get("user_noise"),
                }
        
        elif mode == 'call_chatbot':
            if any([
                turn.get("cooperation_score") is not None,
                turn.get("emotion_score") is not None,
                turn.get("trust_score") is not None,
                turn.get("noise_score") is not None,
            ]):
                processed_turn["predicted_states"] = {
                    "cooperation": turn.get("cooperation_score"),
                    "emotion": turn.get("emotion_score"),
                    "trust": turn.get("trust_score"),
                    "noise": turn.get("noise_score"),
                }
            
            if any([
                turn.get("user_cooperation") is not None,
                turn.get("user_emotion") is not None,
                turn.get("user_trust") is not None,
                turn.get("user_noise") is not None,
            ]):
                processed_turn["real_states"] = {
                    "cooperation": turn.get("user_cooperation"),
                    "emotion": turn.get("user_emotion"),
                    "trust": turn.get("user_trust"),
                    "noise": turn.get("user_noise"),
                }
        
        if turn.get("stage") is not None:
            processed_turn["stage"] = turn.get("stage")
        
        processed_turns.append(processed_turn)
    
    return processed_turns


# ==================== Reward Calculator ====================

class ImprovedMultiTurnDialogueReward:
    """Calculate Dialogue-level Reward"""
    
    def __init__(self, mode='call_chatbot'):
        if mode not in ('call_chatbot', 'call_client'):
            raise ValueError(f"mode must be 'call_chatbot' or 'call_client', got {mode}")
        self.mode = mode
        self.trajectories = []
    
    def add_trajectory(self, trajectory_data: Dict):
        """Add trajectory data"""
        self.trajectories.append(trajectory_data)
    
    def _get_trajectory_completion_status(self, trajectory_data: Dict) -> int:
        """
        Get completion status of a single trajectory.
        CRITICAL: ONLY use user_status (ground truth), NOT model prediction.
        
        Returns:
            1: User confirmed completion
            -1: User hung up
            0: Timeout or no status
        """
        turns = trajectory_data.get("turns", [])
        
        if not turns:
            return 0
        
        # Traverse backwards to find the last turn containing user_status
        for turn in reversed(turns):
            status = turn.get("user_status")
            if status is not None:
                return status
        
        # Return 0 if no user_status is found
        return 0

    @staticmethod
    def _calculate_state_transition_reward(trajectory_data: Dict) -> float:
        """Calculate state transition reward"""
        turns = trajectory_data.get("turns", [])
        if len(turns) < 2:
            return 0.0
        
        first_user_turn = None
        last_user_turn = None
        
        for turn in turns:
            if turn.get("user_cooperation") is not None:
                if first_user_turn is None:
                    first_user_turn = turn
                last_user_turn = turn
        
        if first_user_turn is None or last_user_turn is None:
            return 0.0
        
        cooperation_delta = (last_user_turn.get("user_cooperation", 0) - first_user_turn.get("user_cooperation", 0))
        emotion_delta = (last_user_turn.get("user_emotion", 0) - first_user_turn.get("user_emotion", 0))
        trust_delta = (last_user_turn.get("user_trust", 0) - first_user_turn.get("user_trust", 0))
        
        cooperation_reward = cooperation_delta / 4.0
        emotion_reward = emotion_delta / 3.0
        trust_reward = trust_delta / 5.0
        
        state_transition_reward = (cooperation_reward + emotion_reward + trust_reward) / 3.0
        
        return state_transition_reward

    @staticmethod
    def _calculate_profile_accuracy_reward(trajectory_data: Dict) -> float:
        """Calculate user profile accuracy reward"""
        turns = trajectory_data.get("turns", [])
        if not turns:
            return 0.0
        
        accuracy_scores = []
        
        for turn in turns:
            has_prediction = (
                turn.get("cooperation_score") is not None and
                turn.get("emotion_score") is not None and
                turn.get("trust_score") is not None
            )
            has_real = (
                turn.get("user_cooperation") is not None and
                turn.get("user_emotion") is not None and
                turn.get("user_trust") is not None
            )
            
            if not (has_prediction and has_real):
                continue
            
            coop_diff = abs(turn.get("cooperation_score", 0) - turn.get("user_cooperation", 0))
            emo_diff = abs(turn.get("emotion_score", 0) - turn.get("user_emotion", 0))
            trust_diff = abs(turn.get("trust_score", 0) - turn.get("user_trust", 0))
            
            coop_accuracy = 1.0 - (coop_diff / 4.0)
            emo_accuracy = 1.0 - (emo_diff / 3.0)
            trust_accuracy = 1.0 - (trust_diff / 5.0)
            
            turn_accuracy = (coop_accuracy + emo_accuracy + trust_accuracy) / 3.0
            accuracy_scores.append(turn_accuracy)
        
        if not accuracy_scores:
            return 0.0
        
        return float(np.mean(accuracy_scores))

    def compute_chatbot_reward(self) -> Dict[str, float]:
        """Calculate full chatbot reward"""
        logger.info('[compute_chatbot_reward] Calculating full chatbot reward...')

        if not self.trajectories:
            return {
                'mode': 'call_chatbot',
                'completion_rate': 0.0,
                'false_positive_rate': 0.0,
                'state_transition_reward': 0.0,
                'profile_accuracy_reward': 0.0,
                'total_reward': 0.0,
                'num_trajectories': 0,
                'completed_count': 0,
                'trajectory_rewards': []
            }

        total_count = len(self.trajectories)
        completed_count = 0
        model_success_count = 0
        false_positive_count = 0

        # Statistics: Based on user_status (ground truth completion)
        for traj in self.trajectories:
            user_status = self._get_trajectory_completion_status(traj)
            if user_status == 1:
                completed_count += 1
            
            # Extract model_status (Chatbot's judgment)
            model_statuss = traj.get('model_statuss', [])
            if model_statuss:
                model_status = model_statuss[-1]
            else:
                model_status = 0
            
            # Count FP: Chatbot thinks it's done, but User didn't agree
            if model_status == 1:
                model_success_count += 1
                if user_status != 1:
                    false_positive_count += 1

        batch_completion_rate = completed_count / total_count if total_count > 0 else 0.0
        fpr_rate = (false_positive_count / model_success_count) if model_success_count > 0 else 0.0

        # Calculate state transition rewards
        state_transition_rewards = []
        for traj in self.trajectories:
            reward = self._calculate_state_transition_reward(traj)
            state_transition_rewards.append(reward)

        # Calculate profile accuracy rewards
        profile_accuracy_rewards = []
        for traj in self.trajectories:
            reward = self._calculate_profile_accuracy_reward(traj)
            profile_accuracy_rewards.append(reward)

        # Calculate trajectory-level rewards
        trajectory_rewards = []
        for i in range(total_count):
            traj = self.trajectories[i]
            
            # Use user_status to determine if truly completed
            user_status = self._get_trajectory_completion_status(traj)
            individual_completion = 1.0 if user_status == 1 else 0.0

            traj_state_transition = state_transition_rewards[i]
            traj_profile_accuracy = profile_accuracy_rewards[i]

            # Extract model_status
            model_statuss = traj.get('model_statuss', [])
            model_status = model_statuss[-1] if model_statuss else 0
            
            num_turns = len(traj.get('turns', []))

            # 1. FPR Penalty (False Positive Penalty)
            # Penalty only applied when "User disagrees (user_status != 1)" and "Model hangs up (model_status == 1)"
            if model_status == 1 and user_status != 1:
                fpr_penalty = -10.0  # Severe penalty for false positive
            else:
                fpr_penalty = 0.0

            # 2. Turn Bonus (Multi-turn Reward)
            # Encourage model to chat more; 0.1 points per turn, max 1.5 points (15 turns)
            # Bonus given only if no false positive hang-up occurred
            if fpr_penalty == 0:
                turn_bonus = min(num_turns * 0.1, 1.5)
            else:
                turn_bonus = 0.0  # No turn bonus if false positive hang-up occurred

            # 3. Efficiency Bonus (Efficiency/Success Reward)
            # Big reward for successful task completion
            if individual_completion == 1.0:
                completion_reward = 1.0  # 1 point for successful completion
                # Extra reward if successful and model hung up correctly
                if model_status == 1:
                    efficiency_bonus = 2.0  # Extra 2 points for correct hang-up
                else:
                    efficiency_bonus = 0.0
            else:
                completion_reward = 0.0
                efficiency_bonus = 0.0

            # Reward formula
            # Total Score = Completion + State Improvement + Turn + Efficiency + Penalty
            traj_total = (
                completion_reward + 0
                # 0.5 * traj_state_transition +  # State improvement weight 0.5
                # turn_bonus + 
                # efficiency_bonus +
                # fpr_penalty  # False positive penalty
            )
            
            trajectory_rewards.append({
                'trajectory_id': i,
                'completion_reward': float(completion_reward),
                'state_transition_reward': float(traj_state_transition),
                'profile_accuracy_reward': float(traj_profile_accuracy),
                'turn_bonus': float(turn_bonus),
                'efficiency_bonus': float(efficiency_bonus),
                'fpr_penalty': float(fpr_penalty), 
                'total_reward': float(traj_total)
            })

        avg_state_transition_reward = float(np.mean(state_transition_rewards)) if state_transition_rewards else 0.0
        avg_profile_accuracy_reward = float(np.mean(profile_accuracy_rewards)) if profile_accuracy_rewards else 0.0
        avg_total_reward = float(np.mean([r['total_reward'] for r in trajectory_rewards]))

        print("\n" + "="*80)
        print("[CHATBOT REWARD BREAKDOWN]")
        print("="*80)
        print(f"\nğŸ“Š Batch Level Reward Stats:")
        print(f"  User Completion Rate:    {batch_completion_rate:.4f} ({completed_count}/{total_count})")
        print(f"  Model Prediction Rate:   {model_success_count/total_count:.4f} ({model_success_count}/{total_count})")
        print(f"  False Positive Rate:     {fpr_rate:.4f} ({false_positive_count}/{model_success_count if model_success_count > 0 else 1})")
        print(f"  Avg State Transition:    {avg_state_transition_reward:.4f}")
        print(f"  Avg Profile Accuracy:    {avg_profile_accuracy_reward:.4f}")
        print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"  Avg Total Reward = {avg_total_reward:.4f}")
        
        print(f"\nğŸ“ˆ Trajectory Level Reward Details (Total {total_count}):")
        print(f"  {'ID':<5} {'Comp':<6} {'Trans':<8} {'Turns':<6} {'Eff':<6} {'FPR':<8} {'Total':<8}")
        print(f"  {'-'*70}")
        for r in trajectory_rewards[:10]:
            print(f"  {r['trajectory_id']:<5} {r['completion_reward']:<6.1f} {r['state_transition_reward']:<8.2f} {r['turn_bonus']:<6.1f} {r['efficiency_bonus']:<6.1f} {r['fpr_penalty']:<8.1f} {r['total_reward']:<8.2f}")
        if total_count > 10:
            print(f"  ... ({total_count - 10} more)")
        print("="*80 + "\n")

        return {
            'mode': 'call_chatbot',
            'completion_rate': float(batch_completion_rate),
            'false_positive_rate': float(fpr_rate),
            'state_transition_reward': float(avg_state_transition_reward),
            'profile_accuracy_reward': float(avg_profile_accuracy_reward),
            'total_reward': float(avg_total_reward),
            'num_trajectories': total_count,
            'completed_count': completed_count,
            'trajectory_rewards': trajectory_rewards
        }

    def compute_reward(self) -> Dict[str, float]:
        return self.compute_chatbot_reward()



# ==================== Metric Calculator ====================

class ChatbotMetricCalculator:
    """Chatbot Test Set Metric Calculator"""
    
    def __init__(self):
        self.trajectories = []
    
    def add_trajectory(self, trajectory_data: Dict):
        """Add trajectory data"""
        self.trajectories.append(trajectory_data)
    
    def _check_stage_completeness(self, trajectory_data: Dict) -> bool:
        """
        Check process completeness:
        1. Find the last stage
        2. Check if all preceding stages were experienced in order
        
        Example:
        - [0, 1, 2, 3] â†’ Complete âœ…
        - [0, 1, 2] â†’ Complete âœ… (Last is 2, preceded by 0 and 1)
        - [0, 2, 3] â†’ Incomplete âŒ (Last is 3, but missing 1)
        - [1, 2, 3] â†’ Incomplete âŒ (Missing 0)
        - [0, 1] â†’ Complete âœ… (Last is 1, preceded by 0)
        """
        turns = trajectory_data.get("turns", [])
        
        # Collect all appeared stages
        stages = []
        for turn in turns:
            stage = turn.get("stage")
            if stage is not None:
                stages.append(stage)
        
        if not stages:
            return False
        
        # Find last stage
        last_stage = stages[-1]
        
        # If last stage is 0, return True directly
        if last_stage == 0:
            return True
        
        # Check if all preceding stages were experienced before the last stage
        # e.g., if last is stage 2, check for stage 0 and stage 1
        required_stages = set(range(last_stage))  # {0, 1} if last_stage == 2
        appeared_stages = set(stages)
        
        # Check if all required stages appeared
        return required_stages.issubset(appeared_stages)
    
    def _calculate_att(self, trajectory_data: Dict) -> int:
        """Calculate Average Turns to Target (ATT)"""
        turns = trajectory_data.get("turns", [])
        return len(turns)
    
    def _get_completion_status(self, trajectory_data: Dict) -> int:
        """Get completion status (use user_status as ground truth)"""
        turns = trajectory_data.get("turns", [])
        if not turns:
            return 0
        
        # Use user_status only
        for turn in reversed(turns):
            status = turn.get("user_status")
            if status is not None:
                return status
        
        return 0
    
    def _calculate_emotion_variation(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate emotion variation (last - first)"""
        turns = trajectory_data.get("turns", [])
        
        emotions = []
        for turn in turns:
            emo = turn.get("user_emotion")
            if emo is not None:
                emotions.append(emo)
        
        if len(emotions) < 2:
            return None
        
        return float(emotions[-1] - emotions[0])
    
    def _calculate_emotion_mean(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate user emotion mean"""
        turns = trajectory_data.get("turns", [])
        
        emotions = []
        for turn in turns:
            emo = turn.get("user_emotion")
            if emo is not None:
                emotions.append(emo)
        
        if not emotions:
            return None
        
        return float(np.mean(emotions))
    
    def _calculate_profiling_accuracy(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate user profiling accuracy (1 - MAE)"""
        turns = trajectory_data.get("turns", [])
        
        errors = []
        
        for turn in turns:
            has_prediction = (
                turn.get("cooperation_score") is not None and
                turn.get("emotion_score") is not None and
                turn.get("trust_score") is not None
            )
            has_real = (
                turn.get("user_cooperation") is not None and
                turn.get("user_emotion") is not None and
                turn.get("user_trust") is not None
            )
            
            if not (has_prediction and has_real):
                continue
            
            coop_diff = abs(turn.get("cooperation_score", 0) - turn.get("user_cooperation", 0))
            emo_diff = abs(turn.get("emotion_score", 0) - turn.get("user_emotion", 0))
            trust_diff = abs(turn.get("trust_score", 0) - turn.get("user_trust", 0))
            
            coop_error = coop_diff / 4.0
            emo_error = emo_diff / 3.0
            trust_error = trust_diff / 5.0
            
            turn_error = (coop_error + emo_error + trust_error) / 3.0
            errors.append(turn_error)
        
        if not errors:
            return None
        
        mae = float(np.mean(errors))
        return 1.0 - mae
    
    def _calculate_trust_variation(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate trust variation"""
        turns = trajectory_data.get("turns", [])
        
        trusts = []
        for turn in turns:
            trust = turn.get("user_trust")
            if trust is not None:
                trusts.append(trust)
        
        if len(trusts) < 2:
            return None
        
        return float(trusts[-1] - trusts[0])
    
    def _calculate_trust_mean(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate user trust mean"""
        turns = trajectory_data.get("turns", [])
        
        trusts = []
        for turn in turns:
            trust = turn.get("user_trust")
            if trust is not None:
                trusts.append(trust)
        
        if not trusts:
            return None
        
        return float(np.mean(trusts))
    
    def _calculate_cooperation_variation(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate cooperation variation"""
        turns = trajectory_data.get("turns", [])
        
        cooperations = []
        for turn in turns:
            coop = turn.get("user_cooperation")
            if coop is not None:
                cooperations.append(coop)
        
        if len(cooperations) < 2:
            return None
        
        return float(cooperations[-1] - cooperations[0])
    
    def _calculate_cooperation_mean(self, trajectory_data: Dict) -> Optional[float]:
        """Calculate cooperation mean"""
        turns = trajectory_data.get("turns", [])
        
        cooperations = []
        for turn in turns:
            coop = turn.get("user_cooperation")
            if coop is not None:
                cooperations.append(coop)
        
        if not cooperations:
            return None
        
        return float(np.mean(cooperations))
    
    def compute_metrics(self) -> Dict:
        """Calculate all metrics"""
        if not self.trajectories:
            return {
                'num_trajectories': 0,
                'metrics': {}
            }
        
        total_count = len(self.trajectories)
        
        # L1: Process Compliance
        stage_completeness_list = []
        for traj in self.trajectories:
            is_complete = self._check_stage_completeness(traj)
            stage_completeness_list.append(is_complete)
        
        stage_completeness_rate = sum(stage_completeness_list) / total_count
        
        # L2: Business Capability
        att_list = []
        completed_count = 0
        interrupted_count = 0
        timeout_count = 0
        
        for traj in self.trajectories:
            att = self._calculate_att(traj)
            att_list.append(att)
            
            status = self._get_completion_status(traj)
            if status == 1:
                completed_count += 1
            elif status == -1:
                interrupted_count += 1
            else:
                timeout_count += 1
        
        avg_att = float(np.mean(att_list)) if att_list else 0.0
        completion_rate = completed_count / total_count
        
        # L3: User Experience
        emotion_variations = []
        emotion_means = []
        profiling_accuracies = []
        trust_variations = []
        trust_means = []
        cooperation_variations = []
        cooperation_means = []
        
        for traj in self.trajectories:
            etv = self._calculate_emotion_variation(traj)
            if etv is not None:
                emotion_variations.append(etv)
            
            emo_mean = self._calculate_emotion_mean(traj)
            if emo_mean is not None:
                emotion_means.append(emo_mean)
            
            accuracy = self._calculate_profiling_accuracy(traj)
            if accuracy is not None:
                profiling_accuracies.append(accuracy)
            
            trust_var = self._calculate_trust_variation(traj)
            if trust_var is not None:
                trust_variations.append(trust_var)
            
            trust_mean = self._calculate_trust_mean(traj)
            if trust_mean is not None:
                trust_means.append(trust_mean)
            
            coop_var = self._calculate_cooperation_variation(traj)
            if coop_var is not None:
                cooperation_variations.append(coop_var)
            
            coop_mean = self._calculate_cooperation_mean(traj)
            if coop_mean is not None:
                cooperation_means.append(coop_mean)
        
        metrics = {
            # L1: Process Compliance
            'stage_completeness_rate': float(stage_completeness_rate),
            
            # L2: Business Capability
            'average_turns_to_target': float(avg_att),
            'completion_rate': float(completion_rate),
            'completed_count': completed_count,
            'interrupted_count': interrupted_count,
            'timeout_count': timeout_count,
            
            # L3: User Experience
            'emotion_variation_mean': float(np.mean(emotion_variations)) if emotion_variations else None,
            'emotion_variation_std': float(np.std(emotion_variations, ddof=1)) if len(emotion_variations) > 1 else None,
            'emotion_mean': float(np.mean(emotion_means)) if emotion_means else None,
            'profiling_accuracy': float(np.mean(profiling_accuracies)) if profiling_accuracies else None,
            'profiling_accuracy_std': float(np.std(profiling_accuracies, ddof=1)) if len(profiling_accuracies) > 1 else None,
            'trust_variation_mean': float(np.mean(trust_variations)) if trust_variations else None,
            'trust_variation_std': float(np.std(trust_variations, ddof=1)) if len(trust_variations) > 1 else None,
            'trust_mean': float(np.mean(trust_means)) if trust_means else None,
            'cooperation_variation_mean': float(np.mean(cooperation_variations)) if cooperation_variations else None,
            'cooperation_variation_std': float(np.std(cooperation_variations, ddof=1)) if len(cooperation_variations) > 1 else None,
            'cooperation_mean': float(np.mean(cooperation_means)) if cooperation_means else None,
        }
        
        return {
            'num_trajectories': total_count,
            'metrics': metrics
        }


class ClientMetricCalculator:
    """Client (User Simulator) Test Set Metric Calculator"""
    
    def __init__(self):
        self.trajectories = []
    
    def add_trajectory(self, trajectory_data: Dict):
        """Add trajectory data"""
        self.trajectories.append(trajectory_data)
    
    def _calculate_att(self, trajectory_data: Dict) -> int:
        """Calculate Average Turns to Target"""
        turns = trajectory_data.get("turns", [])
        return len(turns)
    
    def _get_completion_status(self, trajectory_data: Dict) -> int:
        """Get completion status"""
        turns = trajectory_data.get("turns", [])
        if not turns:
            return 0
        
        for turn in reversed(turns):
            status = turn.get("user_status")
            if status is not None:
                return status
        
        return 0
    
    def compute_metrics(self, gpt4_scores: List[Dict]) -> Dict:
        """Calculate all metrics"""
        if not self.trajectories:
            return {'num_trajectories': 0, 'metrics': {}}
        
        total_count = len(self.trajectories)
        
        # L2: Business Capability
        att_list = []
        completed_count = 0
        interrupted_count = 0
        timeout_count = 0
        
        for traj in self.trajectories:
            att = self._calculate_att(traj)
            att_list.append(att)
            
            status = self._get_completion_status(traj)
            if status == 1:
                completed_count += 1
            elif status == -1:
                interrupted_count += 1
            else:
                timeout_count += 1
        
        avg_att = float(np.mean(att_list)) if att_list else 0.0
        completion_rate = completed_count / total_count
        
        # L3: User Experience (Based on LLM evaluation)
        humanness_scores = []
        emotion_rationality_scores = []
        trust_rationality_scores = []
        cooperation_rationality_scores = []
        violation_scores = []
        
        for score in gpt4_scores:
            if score.get('evaluation_status') == 'success':
                humanness_scores.append(score.get('humanness', 3))
                emotion_rationality_scores.append(score.get('emotion_rationality', 3))
                trust_rationality_scores.append(score.get('trust_rationality', 3))
                cooperation_rationality_scores.append(score.get('cooperation_rationality', 3))
                violation_scores.append(score.get('violation', 2))
        
        metrics = {
            # L2: Business Capability
            'average_turns_to_target': float(avg_att),
            'completion_rate': float(completion_rate),
            'completed_count': completed_count,
            'interrupted_count': interrupted_count,
            'timeout_count': timeout_count,
            
            # L3: User Experience (LLM Evaluation)
            'humanness_mean': float(np.mean(humanness_scores)) if humanness_scores else None,
            'humanness_std': float(np.std(humanness_scores, ddof=1)) if len(humanness_scores) > 1 else None,
            'emotion_rationality_mean': float(np.mean(emotion_rationality_scores)) if emotion_rationality_scores else None,
            'emotion_rationality_std': float(np.std(emotion_rationality_scores, ddof=1)) if len(emotion_rationality_scores) > 1 else None,
            'trust_rationality_mean': float(np.mean(trust_rationality_scores)) if trust_rationality_scores else None,
            'trust_rationality_std': float(np.std(trust_rationality_scores, ddof=1)) if len(trust_rationality_scores) > 1 else None,
            'cooperation_rationality_mean': float(np.mean(cooperation_rationality_scores)) if cooperation_rationality_scores else None,
            'cooperation_rationality_std': float(np.std(cooperation_rationality_scores, ddof=1)) if len(cooperation_rationality_scores) > 1 else None,
            'violation_mean': float(np.mean(violation_scores)) if violation_scores else None,
            'violation_std': float(np.std(violation_scores, ddof=1)) if len(violation_scores) > 1 else None,
        }
        
        return {'num_trajectories': total_count, 'metrics': metrics}

# ==================== Version Management Tool ====================

class VersionManager:
    """Manage file version numbers (Pure filename scanning)"""
    
    @staticmethod
    def get_next_version_path(base_path: str) -> str:
        """Get file path for the next version"""
        dir_name = os.path.dirname(base_path)
        file_name = os.path.basename(base_path)
        name_parts = os.path.splitext(file_name)
        base_name = name_parts[0]
        extension = name_parts[1]
        
        max_version = 0
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
            if os.path.exists(dir_name):
                existing_files = os.listdir(dir_name)
                pattern = re.compile(rf'^{re.escape(base_name)}_v(\d+){re.escape(extension)}$')
                
                for f in existing_files:
                    match = pattern.match(f)
                    if match:
                        v = int(match.group(1))
                        max_version = max(max_version, v)
        
        new_version = max_version + 1
        new_file_name = f"{base_name}_v{new_version}{extension}"
        new_path = os.path.join(dir_name, new_file_name) if dir_name else new_file_name
        
        logger.info(f"[VersionManager] Scanned max version: v{max_version}, generating new version: {new_path}")
        
        return new_path
    

    @staticmethod
    def get_current_step_from_dir(eval_dir: str, prefix: str = "chatbot") -> int:
        """Scan existing files in evaluation directory to infer current step"""
        if not os.path.exists(eval_dir):
            return 0
        
        max_step = 0
        pattern = re.compile(rf'{prefix}_val_metric_step(\d+)\.jsonl')
        
        for file_name in os.listdir(eval_dir):
            match = pattern.match(file_name)
            if match:
                step = int(match.group(1))
                max_step = max(max_step, step)
        
        return max_step

# ==================== Main Functions ====================

def compute_score_chatbot(ground_truth_histories: List[List[Dict]],
                          file_path: str = "./outputs/temp_dialog/chatbot_results.json",
                          meta_info: Dict = None) -> Dict:
    """
    Calculate Reward for chatbot mode
    
    Returns:
        {
            'trajectories': [...],
            'summary': {
                'mode': 'call_chatbot',
                'completion_rate': 0.5,
                'false_positive_rate': 0.1,
                'state_transition_reward': 0.2,
                'profile_accuracy_reward': 0.8,
                'total_reward': 1.5,
                'num_trajectories': 100,
                'completed_count': 50,
                'trajectory_rewards': [...]
            }
        }
    """
    results = {'trajectories': [], 'summary': {}}
    reward_calculator = ImprovedMultiTurnDialogueReward(mode='call_chatbot')

    print(f"\n{'='*70}\n[CHATBOT] Processing {len(ground_truth_histories)} trajectories\n{'='*70}\n")
    
    for i, ground_truth_history in enumerate(ground_truth_histories):
        parsed = _parse_ground_truth_history(ground_truth_history)

        # Use user_status to determine final status
        user_status = reward_calculator._get_trajectory_completion_status(parsed)
        
        if user_status == 1:
            completion_status = "completed"
        elif user_status == -1:
            completion_status = "interrupted"
        else:
            completion_status = "timeout"
        
        processed_turns = _process_turns_for_display(parsed["turns"], mode='call_chatbot')

        traj_result = {
            "trajectory_id": i,
            "parse_status": parsed["parse_status"],
            "num_turns": len(parsed["turns"]),
            "raw_statuss": parsed["raw_statuss"],
            "model_statuss": parsed["model_statuss"],
            "turns": processed_turns,
            "final_status": user_status,
            "completion_status": completion_status,
            "mode": "call_chatbot",
            "raw_trajectory": ground_truth_history,
        }
        results['trajectories'].append(traj_result)
        reward_calculator.add_trajectory(parsed)

    reward_result = reward_calculator.compute_chatbot_reward()
    results['summary'] = reward_result
    
    if 'trajectory_rewards' in reward_result:
        for i, traj_reward in enumerate(reward_result['trajectory_rewards']):
            if i < len(results['trajectories']):
                results['trajectories'][i]['reward_breakdown'] = traj_reward
    
    print(f"\n{'='*70}")
    print(f"Batch Completion Rate: {reward_result['completion_rate']:.2%}")
    fpr = reward_result.get('false_positive_rate', 0.0)
    print(f"False Positive Rate (FPR): {fpr:.2%}")
    print(f"Avg State Transition Reward: {reward_result['state_transition_reward']:.4f}")
    print(f"Avg Profile Accuracy Reward: {reward_result['profile_accuracy_reward']:.4f}")
    print(f"Avg Total Reward: {reward_result['total_reward']:.4f}")
    print(f"{'='*70}\n")
    
    if file_path:
        try:
            versioned_path = VersionManager.get_next_version_path(file_path)
            os.makedirs(os.path.dirname(versioned_path) or '.', exist_ok=True)
            with open(versioned_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"âœ“ Results saved to: {versioned_path}\n")
        except Exception as e:
            print(f"âœ— Failed to save results: {e}\n")
    
    return results

def compute_metric_chatbot(ground_truth_histories: List[List[Dict]],
                           current_step: int = None,
                           meta_info: Dict = None,
                           file_path: str = None) -> Dict:
    """Calculate evaluation metrics for Chatbot test set"""
    # 1. Determine current_step
    if current_step is None:
        if meta_info and 'current_step' in meta_info:
            current_step = meta_info['current_step']
        else:
            eval_dir = "./outputs/evaluation"
            current_step = VersionManager.get_current_step_from_dir(eval_dir, prefix="chatbot") + 1
    
    print(f"\n{'='*80}")
    print(f"[CHATBOT TEST SET EVALUATION] Step {current_step}")
    print(f"{'='*80}\n")
    print(f"Processing {len(ground_truth_histories)} test trajectories...\n")
    
    # 2. Parse trajectories
    metric_calculator = ChatbotMetricCalculator()
    parsed_trajectories = []
    
    # Calculate Confusion Matrix
    true_positive_count = 0   # TP: Chatbot thinks completed, User agrees
    false_positive_count = 0  # FP: Chatbot thinks completed, User disagrees (False Positive)
    true_negative_count = 0   # TN: Chatbot thinks incomplete, User disagrees
    false_negative_count = 0  # FN: Chatbot thinks incomplete, User agrees
    
    for i, ground_truth_history in enumerate(ground_truth_histories):
        parsed = _parse_ground_truth_history(ground_truth_history)
        metric_calculator.add_trajectory(parsed)
        parsed_trajectories.append(parsed)

        # Extract status from parsed data
        model_statuss = parsed.get("model_statuss", [])
        raw_statuss = parsed.get("raw_statuss", [])
        
        # Ensure index is not out of bounds
        if model_statuss and raw_statuss:
            # Get the last status (ensure both lists have same length)
            min_len = min(len(model_statuss), len(raw_statuss))
            if min_len > 0:
                model_status = model_statuss[min_len - 1]
                user_status = raw_statuss[min_len - 1]
            else:
                model_status = 0
                user_status = 0
        else:
            model_status = 0
            user_status = 0
        
        # Calculate Confusion Matrix
        if model_status == 1 and user_status == 1:
            true_positive_count += 1   # TP
        elif model_status == 1 and user_status != 1:
            false_positive_count += 1  # FP (False Positive)
        elif model_status != 1 and user_status != 1:
            true_negative_count += 1   # TN
        elif model_status != 1 and user_status == 1:
            false_negative_count += 1  # FN (False Negative)
        
        if (i + 1) % 10 == 0 or i == 0:
            print(f"âœ“ Parsed {i + 1}/{len(ground_truth_histories)} trajectories")
    
    # 3. Calculate metrics
    print(f"\nCalculating metrics...")
    result = metric_calculator.compute_metrics()
    metrics = result['metrics']
    
    # Correctly calculate FPR
    actual_negative_count = false_positive_count + true_negative_count
    fpr_rate = (false_positive_count / actual_negative_count) if actual_negative_count > 0 else 0.0
    
    # Calculate other useful metrics
    actual_positive_count = true_positive_count + false_negative_count
    tpr_rate = (true_positive_count / actual_positive_count) if actual_positive_count > 0 else 0.0  # Recall
    precision = (true_positive_count / (true_positive_count + false_positive_count)) if (true_positive_count + false_positive_count) > 0 else 0.0
    
    # Inject metrics
    metrics['false_positive_rate'] = fpr_rate
    metrics['true_positive_rate'] = tpr_rate  # Recall/Sensitivity
    metrics['precision'] = precision
    metrics['confusion_matrix'] = {
        'TP': true_positive_count,
        'FP': false_positive_count,
        'TN': true_negative_count,
        'FN': false_negative_count
    }

    # 4. Print metrics
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Chatbot Test Set Metrics (Step {current_step})")
    print(f"{'='*80}\n")
    
    print(f"ã€L1: Process Complianceã€‘")
    print(f"  Stage Completeness: {metrics['stage_completeness_rate']:.2%}")
    
    print(f"\nã€L2: Business Capabilityã€‘")
    print(f"  Avg Turns to Target (ATT):       {metrics['average_turns_to_target']:.2f} turns")
    print(f"  Batch Completion Rate (CR):       {metrics['completion_rate']:.2%}")
    print(f"  False Positive Rate (FPR):        {fpr_rate:.2%} ({false_positive_count}/{actual_negative_count})")
    print(f"  True Positive Rate (TPR/Recall):  {tpr_rate:.2%} ({true_positive_count}/{actual_positive_count})")
    print(f"  Precision:                        {precision:.2%} ({true_positive_count}/{true_positive_count + false_positive_count})")
    print(f"\n  Confusion Matrix:")
    print(f"    - TP (Chatbotâœ“ Userâœ“): {true_positive_count}")
    print(f"    - FP (Chatbotâœ“ Userâœ—): {false_positive_count}  â† False Positive")
    print(f"    - TN (Chatbotâœ— Userâœ—): {true_negative_count}")
    print(f"    - FN (Chatbotâœ— Userâœ“): {false_negative_count}  â† False Negative")
    print(f"\n  Status Breakdown:")
    print(f"    - Completed (final_status=1):  {metrics['completed_count']}")
    print(f"    - Interrupted (final_status=-1): {metrics['interrupted_count']}")
    print(f"    - Timeout (final_status=0):     {metrics['timeout_count']}")
    
    print(f"\nã€L3: User Experienceã€‘")
    if metrics['emotion_variation_mean'] is not None:
        if metrics['emotion_variation_std'] is not None:
            print(f"  Emotion Trajectory Variation (ETV): {metrics['emotion_variation_mean']:+.3f} Â± {metrics['emotion_variation_std']:.3f}")
        else:
            print(f"  Emotion Trajectory Variation (ETV): {metrics['emotion_variation_mean']:+.3f}")
    
    if metrics['emotion_mean'] is not None:
        print(f"  User Emotion Mean:                {metrics['emotion_mean']:.3f}")
    
    if metrics['profiling_accuracy'] is not None:
        if metrics['profiling_accuracy_std'] is not None:
            print(f"  User Profiling Accuracy:          {metrics['profiling_accuracy']:.4f} Â± {metrics['profiling_accuracy_std']:.4f}")
        else:
            print(f"  User Profiling Accuracy:          {metrics['profiling_accuracy']:.4f}")
    
    if metrics['trust_variation_mean'] is not None:
        if metrics['trust_variation_std'] is not None:
            print(f"  Trust Variation:                  {metrics['trust_variation_mean']:+.3f} Â± {metrics['trust_variation_std']:.3f}")
        else:
            print(f"  Trust Variation:                  {metrics['trust_variation_mean']:+.3f}")
    
    if metrics['trust_mean'] is not None:
        print(f"  User Trust Mean:                  {metrics['trust_mean']:.3f}")
    
    if metrics['cooperation_variation_mean'] is not None:
        if metrics['cooperation_variation_std'] is not None:
            print(f"  Cooperation Variation:            {metrics['cooperation_variation_mean']:+.3f} Â± {metrics['cooperation_variation_std']:.3f}")
        else:
            print(f"  Cooperation Variation:            {metrics['cooperation_variation_mean']:+.3f}")
    
    if metrics['cooperation_mean'] is not None:
        print(f"  Cooperation Mean:                 {metrics['cooperation_mean']:.3f}")
    
    print(f"\n{'='*80}\n")
    
    # 5. Save metrics to evaluation directory
    eval_dir = "./outputs/evaluation"
    os.makedirs(eval_dir, exist_ok=True)
    
    output_file = os.path.join(eval_dir, f"chatbot_val_metric_step{current_step}.jsonl")
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            output_data = {
                'step': current_step,
                'num_trajectories': result['num_trajectories'],
                'metrics': metrics
            }
            f.write(json.dumps(output_data, ensure_ascii=False) + '\n')
        
        print(f"âœ“ Metrics saved to: {output_file}\n")
    except Exception as e:
        print(f"âœ— Failed to save metrics: {e}\n")
    
    # 6. Save trajectory details to temp_dialog directory (if file_path is specified)
    if file_path:
        try:
            versioned_path = VersionManager.get_next_version_path(file_path)
            os.makedirs(os.path.dirname(versioned_path) or '.', exist_ok=True)
            
            # Construct trajectory details
            trajectories_detail = []
            for i, parsed in enumerate(parsed_trajectories):
                turns = parsed.get("turns", [])
                
                # Extract final_status from turns
                final_status = 0
                if turns:
                    for turn in reversed(turns):
                        status = turn.get("user_status")
                        if status is not None:
                            final_status = status
                            break
                
                if final_status == 1:
                    completion_status = "completed"
                elif final_status == -1:
                    completion_status = "interrupted"
                else:
                    completion_status = "timeout"
                
                processed_turns = _process_turns_for_display(parsed["turns"], mode='call_chatbot')
                
                # Extract model_statuss (ensure correct length)
                model_statuss = parsed.get("model_statuss", [])
                raw_statuss = parsed.get("raw_statuss", [])
                valid_length = min(len(turns), len(model_statuss), len(raw_statuss))
                
                traj_result = {
                    "trajectory_id": i,
                    "parse_status": parsed["parse_status"],
                    "num_turns": len(parsed["turns"]),
                    "raw_statuss": raw_statuss[:valid_length],
                    "model_statuss": model_statuss[:valid_length],
                    "turns": processed_turns,
                    "final_status": final_status,
                    "completion_status": completion_status,
                    "mode": "val_chatbot",
                    "raw_trajectory": ground_truth_histories[i]
                }
                trajectories_detail.append(traj_result)
            
            # Save
            with open(versioned_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'trajectories': trajectories_detail,
                    'summary': {
                        'step': current_step,
                        'num_trajectories': result['num_trajectories'],
                        'metrics': metrics
                    }
                }, f, ensure_ascii=False, indent=2)
            
            print(f"âœ“ Trajectory details saved to: {versioned_path}\n")
        except Exception as e:
            print(f"âœ— Failed to save trajectory details: {e}\n")
    
    return {
        'step': current_step,
        'num_trajectories': result['num_trajectories'],
        'metrics': metrics
    }